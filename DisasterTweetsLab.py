# -*- coding: utf-8 -*-
"""DisasterTweets.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SBx8vbbwDRvLb1pGYS_uFdADhZ9AgEsC

# Classifiying Tweets - Kaggle Competiton
The *Natural Language Processing with Disaster Tweets* hosted on Kaggle is meant to give data scientist a manageable exercise to practice machine learning skills on a classifacation problem. The dataset, provided by 'Data for Everyone', contains a training and test set of tweets.

The objective of the competition is to classify the tweets contained in the test set as being related to disasters versus non-disater related tweets. The training set contains 10,000 hand-classified Tweets while the test set contains 3,243 unclassified tweets. Below you can find my code that implements a recurrent neural network architecture.

Submissions are scored using the F1 score.

## Import Libraries
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files, drive


import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, SimpleRNN, Dropout, Bidirectional, GRU, BatchNormalization
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import Callback
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger
from tensorflow.keras import regularizers


drive_path = '/content/drive'
# Check if Google Drive is mounted and if not, mount it
if not os.path.ismount(drive_path):
    from google.colab import drive
    drive.mount(drive_path)
else:
    print("Google Drive is already mounted")

"""Import the dataset from Kaggle"""

#Download Kaggle dataset
!pip install Kaggle #install kaggle API

#uplaod your Kaggle API key
files.upload()

#Kaggle API key
!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c nlp-getting-started

!unzip -o nlp-getting-started.zip

"""Read in dataset into dateframes"""

dataframes = {}

for filename in os.listdir('./'):
    if filename.endswith('.csv'):
        df_name = os.path.splitext(filename)[0]
        dataframes[df_name] = pd.read_csv(filename)

for name, df in dataframes.items():
    print(f"Loaded {name} with shape {df.shape}")
    globals()[name] = df

"""Make some directories theat we'll use later.


"""

!mkdir -p 'models'
!mkdir -p 'logs'

"""# EDA
Let's see what the training data looks like.
"""

print(train.head(),"\n")

"""The training set has 7,613 total observations with target labels provided.

"""

print(train.info(),"\n")

"""Let's take a look at the length of text strings and how many words appear in each string."""

bins = 25

train['text_length'] = train['text'].apply(len)

plt.hist(train['text_length'], bins = bins)
plt.xlabel('Text Length')
plt.ylabel("Frequency")
plt.show()

train['word_count'] = train['text'].apply(lambda x: len(x.split()))

plt.hist(train['word_count'], bins = bins)
plt.xlabel('Word Count')
plt.ylabel("Frequency")
plt.show()

"""## Balancing the Training Data

We need to make sure the labeled data is balanced or else bias may be introduced. To explain, if one class has significantly fewer instances than another, then the model may develop a bias towards the majority class. In this case, incorrect predictions on the minority class could still give high model accuracy.
"""

print(train.describe())

"""It looks like there are slightly more training instances labeled as '0'. Let's visualize this further:"""

plt.hist(train['target'], bins=np.arange(-0.5, 2, 1), edgecolor='black', alpha=0.7, rwidth=0.8)
plt.xticks([0, 1], ['Not Disaster (0)', 'Disaster (1)'])
plt.ylabel('Frequency')
plt.title('Distribution of Target Labels')
plt.show()

#Balance the training data.

disasters = train[train['target'] == 1]
not_disasters = train[train['target'] == 0 ].sample(n=len(disasters), random_state=42)
balanced_train = pd.concat([disasters, not_disasters])
balanced_train.describe()

"""# Model Training

Split the training set into a training and validation set.
"""

X_train, X_valid, y_train, y_valid = train_test_split(balanced_train['text'], balanced_train['target'], shuffle = True, test_size=0.2, random_state=42)
train_data = pd.DataFrame({'text': X_train, 'label': y_train})
valid_data = pd.DataFrame({'text': X_valid, 'label': y_valid})

print(train_data.info(),"\n")
print(valid_data.info(),"\n")

print("Total Tweets =",valid_data.shape[0]+train_data.shape[0])

"""Set up a function to train an RNN on the data. This function can be easily accesed repeatedly for hyperparameter tuning."""

def train_RNN(param_combo, num_words, output_dim, lr, dropout_rate, batch_size, train_len, epochs):
  tokenizer = Tokenizer(num_words= num_words, oov_token="<OOV>")
  tokenizer.fit_on_texts(train_data['text'])  # Fit the tokenizer on the training data
  # vocab_size = len(tokenizer.word_index) + 1 #total words seen in the data
  # print(vocab_size)

  # Tokenize the training and validation data
  train_sequences = tokenizer.texts_to_sequences(train_data['text'])
  valid_sequences = tokenizer.texts_to_sequences(valid_data['text'])

  # print(train_sequences)
  train_lengths = [len(seq) for seq in train_sequences]
  max_length = int(np.percentile(train_lengths, 95))

  # print(max_length)
  train_padded = pad_sequences(train_sequences, maxlen=max_length, padding='post')
  valid_padded = pad_sequences(valid_sequences, maxlen=max_length, padding='post')

  # print(train_padded)

  tf.keras.backend.clear_session()
  model = Sequential([
      Embedding(input_dim=num_words, output_dim=output_dim),
      Bidirectional(LSTM(64, return_sequences=True, kernel_regularizer=regularizers.l2(0.1))),
      Dropout(dropout_rate),
      Bidirectional(LSTM(128, return_sequences=False)),
      Dropout(dropout_rate),
      Dense(units=1, activation='sigmoid')
  ])

  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate= lr,
                                                            decay_steps=train_len//batch_size, # Apply decay after each epoch
                                                            decay_rate=0.95)

  model.compile(
      optimizer=Adam(learning_rate = lr_schedule),
      loss='binary_crossentropy',
      metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]
  )

  callbacks = [
      EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True),
      ModelCheckpoint(filepath = f'/content/models/model_{param_combo}.keras', monitor = 'val_loss', save_best_only = True),
      CSVLogger(filename = f'/content/logs/model_{param_combo}.csv', separator = ',', append = False)
  ]

  history = model.fit(
    x = train_padded,
    y = train_data['label'],
    validation_data = (valid_padded, valid_data['label']),
    epochs = epochs,
    batch_size = int(batch_size),
    shuffle = True,
    callbacks = callbacks,
    )

  epochs_run = len(history.history['loss'])
  best_accuracy = max(history.history['val_accuracy'])

  new_row = pd.DataFrame({
    'param_combo' : [param_combo],
    'learning_rate': [lr],
    'dropout_rate': [dropout_rate],
    'epochs run': [epochs_run],
    'ending_lr': model.optimizer.learning_rate.numpy(),
    'num_words': [num_words],
    'output_dim': [output_dim],
    'batch_size': [batch_size],
    'val_accuracy': [best_accuracy],
  })

  return model, new_row

results_df = pd.DataFrame()

"""## Hyperparameter Tuning
Below we tune the model with a small amount of hyperparameters with only two values each. This approach was implemented for the sake of simplicity and time. In further work, many more parameters and model architectures could be investaged. The hyperparameters tuned here inlcude:
* **num_words** - Sets the maximum number of most frequent words retained in the vocabulary, influencing the input data's vectorization
* **lr** (learning rate) - Determines the step size at each iteration to minimize the loss function; critical for the convergence speed and quality
* **output_dim** - Defines the size of the vector space for word embeddings, affecting how text features are represented
* **dropout_rate**  - Specifies the fraction of input units to drop during training, helping prevent overfitting by making the model less sensitive to specific features
* **batch_size** - Controls the number of training examples used per iteration, impacting training speed and model stability

### Network Architecture
Although not shown here, various RNN architectures were evaluated including those that used GRU layers, unidirectional LSTM layers, and various layer sizes. Ulitmately I chose to use this structure for the analysis. Other structures did not appear to provide substatial improvement in results.
"""

test_num_words = [10000, 15000]
test_lr = [0.001, 0.0001]
test_output_dim = [32,64]
test_dropout_rate = [0.2,0.5]
batch_size = [64,128]
train_len = len(train_data)

i = 0

for num_words_ in test_num_words:
  for lr_ in test_lr:
    for output_dim_ in test_output_dim:
      for dropout_rate_ in test_dropout_rate:
        for batch_size_ in batch_size:
          i += 1
          print("*** PARAMETER TEST:",i,"***")
          print("num_words:",num_words_)
          print("lr:",lr_)
          print("output_dim:",output_dim_)
          print("dropout_rate:",dropout_rate_)
          print("batch_size:",batch_size_)
          model, new_res = train_RNN(param_combo = i,
                              num_words = num_words_,
                              output_dim = output_dim_,
                              lr = lr_,
                              dropout_rate = dropout_rate_,
                              batch_size = batch_size_,
                              train_len = train_len,
                              epochs = 5)

          results_df = pd.concat([results_df, new_res], ignore_index=True)

"""Let's look at the hyperparameter tuning results"""

results_df[0:i]

range = min(results_df['val_accuracy']), max(results_df['val_accuracy'])
print(range)

"""### Tuning Results

We can see a general trend the individual hyperparameter tuning model results. Most parameter cobinations begin with a lower training accuracy that increases to above 90% by the end of five epochs while the validation accuracy begins in 50-70% range and plateus in the upper 70s. This shows that the selected hyperparameter combinations lead the model to overfit after one epoch in many cases.
"""

results_df.to_csv('hyperparameter_tuning_results.csv', index=False)
results_df.to_csv('/content/drive/MyDrive/DTSA 5511 - Deep Learning/Week 4 - Lab/hyperparameter_tuning_results.csv', index=False)

"""Store the best performing parameters"""

# Get the best row based on max accuracy
best_params = results_df.loc[results_df['val_accuracy'].idxmax()]
print(best_params)

# Extract values into named variables
best_param_combo = best_params['param_combo']
best_num_words = best_params['num_words']
best_output_dim = best_params['output_dim']
best_lr = best_params['learning_rate']
best_dropout_rate = best_params['dropout_rate']
best_batch_size = best_params['batch_size']

"""Retrain a model with the optimal parameters to take a closer look"""

final_train_results = pd.DataFrame()
model, results = train_RNN(param_combo = 'final',
                  num_words = int(best_num_words),
                  output_dim = int(best_output_dim),
                  lr = best_lr,
                  dropout_rate = best_dropout_rate,
                  batch_size = best_batch_size,
                  train_len = train_len,
                  epochs = 10
                  )
final_train_results = pd.concat([final_train_results, results], ignore_index=True)
final_train_results.to_csv('final_train_results.csv', index=False)
final_train_results.head()

"""## Training Results

The RNN model was trained using a specific combination of hyperparameters, labeled as Parameter Combination 11. It started with an initial learning rate of 0.001, which decayed to 0.0007 by the end of training. The model utilized a vocabulary size of 10,000 words and an output dimension of 64 for embeddings, with a dropout rate set at 0.5 to prevent overfitting. The batch size for training was 64. These settings resulted in the model achieving an accuracy of 77.9%.

After parameter tuning as complete, I decided to retrain the model over more epochs, increasing them to 10. This training gave slightly different, but overall similar results. This final model is used to make predictions.

Prepare the test data for making predictions
"""

# Get token sequences for the test data
tokenizer = Tokenizer(num_words= best_num_words, oov_token="<OOV>")
tokenizer.fit_on_texts(train_data['text'])

# Tokenize the training and validation data
test_sequences = tokenizer.texts_to_sequences(test['text'])

test_lengths = [len(seq) for seq in test_sequences]
max_length = int(np.percentile(test_lengths, 95))

# print(max_length)
test_padded = pad_sequences(test_sequences, maxlen=max_length, padding='post')

"""Make predictions on the Kaggle test set"""

predictions = model.predict(test_padded)

predictions = np.where(predictions >= 0.5, 1, 0)

submission = pd.DataFrame({'id': test['id'], 'target': predictions.flatten()})
submission.to_csv('submission.csv', index=False)
submission.head()

!kaggle competitions submit -c nlp-getting-started -f submission.csv -m "Message"

"""Here's my submission results on Kaggle:"""

!pip install gdown
import gdown
import matplotlib.pyplot as plt
import matplotlib.image as mpimg

url = 'https://drive.google.com/uc?id=1QLVEsu_FIxVX-8bUvzIutBpSHk_XRmOI'

output = 'downloaded_image.jpg'

gdown.download(url, output, quiet=False)

img = mpimg.imread(output)
plt.figure(figsize=(10,8))
plt.imshow(img)
plt.axis('off')
plt.show()

"""# Conclusion & Future Work
With a relatively simple recurrent neural network architecture, the model was able to achieve an F1 score of approximately 0.78. Many of the models gave similar results during training, with

# Citation
Addison Howard, devrishi, Phil Culliton, and Yufeng Guo. Natural Language Processing with Disaster Tweets. https://kaggle.com/competitions/nlp-getting-started, 2019. Kaggle.
"""